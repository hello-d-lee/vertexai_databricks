{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2a9c90-1975-4c39-ad3e-879d8192c4b9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this section you will read in the same dataset that we've used in the other two exercises into a dataframe in Databricks, train an scikit-learn logistic regression model, and then export that model from the MLFlow model registry to the Vertex AI model registry so that it can then be used for prediction activities. \n",
    "\n",
    "## Setup\n",
    "1. First, create a new Databricks notebook in python\n",
    "2. You may need to start the cluster you created earlier, and attach the new notebook to the cluster\n",
    "3. Install some additional libraries onto the Databricks cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aac037-00a2-4926-af8d-32afc0689fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install category_encoders\n",
    "%pip install databricks_registry_webhooks\n",
    "%pip install db-dtypes\n",
    "%pip install google-cloud-mlflow\n",
    "%pip install google-cloud-aiplatform\n",
    "%pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27de30-94fb-4b41-9a02-b99d9545ccdb",
   "metadata": {},
   "source": [
    "5. You will need to restart the cluster after these packages have been installed\n",
    "4. Next, import the required libraries - remember that if the cluster turns off, you'll need to rerun these imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0469697-4a0c-4713-a538-ad88e7a0e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.mllib import *\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import *\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import mlflow\n",
    "import bamboolib as bam\n",
    "import os\n",
    "import urllib\n",
    "import google.oauth2.id_token\n",
    "import google.auth.transport.requests\n",
    "from databricks_registry_webhooks import RegistryWebhooksClient, HttpUrlSpec\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "#from google.cloud import aiplatform as vertex_ai\n",
    "from mlflow import deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977deb92-8fcc-4559-ae34-94fd917b598d",
   "metadata": {},
   "source": [
    "7. Next, define some variables & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0122f0-4886-4bb0-aa01-2de501704217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth function - required to be used when making request to deploy model\n",
    "def make_authorized_get_request(endpoint, audience):\n",
    "    \"\"\"\n",
    "    Make an authorized request to the given endpoint.\n",
    "    Args:\n",
    "        endpoint: The endpoint to send the request to.\n",
    "        audience: The audience to use when validating the JWT.\n",
    "    Returns:\n",
    "        The JSON response from the request.\n",
    "    \"\"\"\n",
    "    # Define the request.\n",
    "    req = urllib.request.Request(endpoint)\n",
    "\n",
    "    # Get the ID token from the environment.\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    id_token = google.oauth2.id_token.fetch_id_token(auth_req, audience)\n",
    "    \n",
    "    return id_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b92412-e090-41cc-a90d-b30853c41b76",
   "metadata": {},
   "source": [
    "8. Depending on how your GCP / Databricks project environment has been configured to communicate, you may need to generate an authorization token so that Databricks can read in data from BigQuery\n",
    "\n",
    "* https://github.com/GoogleCloudDataproc/spark-bigquery-connector#how-do-i-authenticate-outside-gce--dataproc\n",
    "* if you go back to the Google Cloud console and click on \"activate cloud shell\" in the upper righthand corner, you can generate a token by running the following gcloud command:\n",
    "gcloud auth application-default print-access-token\n",
    "![](./gcloud_auth.png)\n",
    "\n",
    "9. You'll then need to copy the printed access token, and go back to your Databricks notebook and paste this into the a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615234-4c0f-43ac-b9da-3bc9ea100093",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcpAccessToken=\"your-access-token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaf63f-dc6d-4c56-b372-062f93659c08",
   "metadata": {},
   "source": [
    "10. In order to move models using the mlflow google cloud plugin, you'll need to specify a Google Cloud Storage staging bucket for artifacts to be stored as part of the webhook function. If you don't have an existing bucket you want to use, you can create one. \n",
    "* In the GCP cloud console, navigate to Cloud Storage\n",
    "* Select Create Bucket\n",
    "* Make sure to use the same region where we've been creating all of our other resources (e.g. us-central1)\n",
    "* Keep the default settings \n",
    "* Navigate to the bucket, and create a folder called \"models\"\n",
    "![](./bucket_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062a956-6f78-4415-9daf-29adb3a1c79c",
   "metadata": {},
   "source": [
    "11. Next, go back to your Databricks notebook. Let's define some variables. Make sure to use your project, the same region that you have been using to store the models in the Vertex AI model registry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c37126-8e63-4b59-8866-a7cc39a3fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE TO REGISTER DIFFERENT MODEL USING MLFLOW API\n",
    "bucket_uri = \"gs://bq-logit-regression-demo\"\n",
    "models_uri = f\"{bucket_uri}/models/\"\n",
    "\n",
    "# VERTEX AI SETTINGS\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = \"leedeb-experimentation\"\n",
    "FUNCTION_NAME = \"deploy\"\n",
    "CLOUD_FUNCTION_URL = f\"https://{REGION}-{PROJECT_ID}.cloudfunctions.net/{FUNCTION_NAME}\"\n",
    "CREDENTIAL_PATH = \"/dbfs/FileStore/tables/vertexdatabricks/leedeb_experimentation_dbd17a0139eb.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = CREDENTIAL_PATH\n",
    "\n",
    "# WHEN STREAMING DATASET IN SPECIFY MODEL STAGE TO LOAD IN\n",
    "# model_stage=\"Production\"\n",
    "\n",
    "# set registry GCS bucket URI\n",
    "mlflow.set_registry_uri(models_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909c35b-b4af-4485-9b48-bbda0d90aa0c",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "12. Read the data in from BigQuery using the spark connector to read it into a Spark dataframe. Make sure to update the full_table_name variable for your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f9652-193f-4315-a6f2-8bdfdb756ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from BQ into spark df\n",
    "full_table_name=\"leedeb-experimentation.bq_databricks_vertex.training_data\"\n",
    "\n",
    "df = spark.read.format(\"bigquery\") \\\n",
    "        .option(\"gcpAccessToken\", gcpAccessToken) \\\n",
    "        .option(\"inferSchema\" , \"true\") \\\n",
    "        .option(\"table\",full_table_name).load()\n",
    "\n",
    "df.createOrReplaceTempView(\"training_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921be6b-5fcf-416e-a921-8bdc18c58e8e",
   "metadata": {},
   "source": [
    "13. Since we'll be using scikit-learn, convert from a spark dataframe to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabf1db-77ac-4cd2-9427-36846af825f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df.toPandas()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a10e73-e952-49e7-85c8-b523dbdb539f",
   "metadata": {},
   "source": [
    "14. Preprocess the data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a7715-9de9-4262-999c-b5599504111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "X = train_data.drop(columns='label', axis=1) \n",
    "y = train_data.label.values\n",
    "\n",
    "# use binary encoding to encode two categorical features\n",
    "enc = BinaryEncoder().fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "numeric_dataset = enc.transform(X)\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_dataset, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bed4a-ee2a-44e9-ae93-a8226b22dc2f",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "15. Train the model and log the run to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34b905-163c-4106-94bd-13ec055ee3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    # define model\n",
    "    scaler = StandardScaler()\n",
    "    lr = LogisticRegression()\n",
    "    model = Pipeline([('standardize', scaler),\n",
    "                        ('log_reg', lr)])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # mlflow.end_run()\n",
    "    run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a728ce-bdcd-4dc4-b155-6eba6f3bed6c",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "16. Evaluate the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c08ff3-c66a-4126-937b-e27cdd8fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model - training score\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_train_hat_probs = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_hat)*100\n",
    "train_auc_roc = roc_auc_score(y_train, y_train_hat_probs)*100\n",
    "\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_train, y_train_hat))\n",
    "\n",
    "print('Training AUC: %.4f %%' % train_auc_roc)\n",
    "\n",
    "print('Training accuracy: %.4f %%' % train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a944be6-84db-476b-bd03-bf869c3bef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model - testing score\n",
    "y_test_hat = model.predict(X_test)\n",
    "y_test_hat_probs = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_hat)*100\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_hat_probs)*100\n",
    "\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_test_hat))\n",
    "\n",
    "print('Testing AUC: %.4f %%' % test_auc_roc)\n",
    "\n",
    "print('Testing accuracy: %.4f %%' % test_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9122570-a9ae-40e2-9803-1b775e8db536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check precision and recall\n",
    "print(classification_report(y_test, y_test_hat, digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539d752-97c2-4f0c-9b4b-49bad89d325c",
   "metadata": {},
   "source": [
    "## Register model in MLFlow & prepare for deployment to Vertex\n",
    "17. In order to use the model for online prediction, we need to import the model into the Vertex AI model registry. First, the model needs to be registered in MLFlow staging environment. We create a webhook with the mlflow google cloud ai platform plugin. When the model is moved from Stage to Production in MLFlow, this triggers a deployment to the model registry to an endpoint in Vertex AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a3a2d-f3f7-494f-ad54-8052c286692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mlflow run id\n",
    "runID = run.info.run_uuid\n",
    "runID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5575a19-7059-4a39-bb52-474d8276b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model in mlflow model registry\n",
    "import uuid\n",
    "id = uuid.uuid4().hex[:10]\n",
    "model_name_mlflow = 'manually_registered_sklearn'\n",
    "artifact_path='model'\n",
    "model_uri = \"runs:/{run_id}/artifacts/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=model_name_mlflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8734687-b0c2-4ec8-85bb-06a563c52820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n",
    " \n",
    "def wait_until_ready(model_name, model_version):\n",
    "  client = MlflowClient()\n",
    "  for _ in range(10):\n",
    "    model_version_details = client.get_model_version(\n",
    "      name=model_name,\n",
    "      version=model_version,\n",
    "    )\n",
    "    status = ModelVersionStatus.from_string(model_version_details.status)\n",
    "    print(\"Model status: %s\" % ModelVersionStatus.to_string(status))\n",
    "    if status == ModelVersionStatus.READY:\n",
    "      break\n",
    "    time.sleep(1)\n",
    "  \n",
    "wait_until_ready(model_details.name, model_details.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7ecef-9691-4762-b3d7-b3dd518709c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a version specific description\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "client.update_registered_model(\n",
    "  name=model_details.name,\n",
    "  description=\"This classification model version was built using data from BigQuery.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042baec-169b-4770-ad81-a8f55f890d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id  = make_authorized_get_request(CLOUD_FUNCTION_URL, CLOUD_FUNCTION_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dede0b-beaf-4a75-abef-98abaac932fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url_spec = HttpUrlSpec(\n",
    "  url=CLOUD_FUNCTION_URL,\n",
    "  authorization=f\"Bearer {token_id}\"\n",
    ")\n",
    "http_webhook = RegistryWebhooksClient().create_webhook(\n",
    "  model_name=model_details.name,\n",
    "  events=[\"MODEL_VERSION_TRANSITIONED_STAGE\"],\n",
    "  http_url_spec=http_url_spec,\n",
    "  description=\"Testing deploy model\",\n",
    "  status=\"TEST_MODE\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe25f5b-2c95-4c68-b98c-3ca54fa49dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_webhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45486e-9abc-40c6-8aa1-0b8db21f0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_webhook = RegistryWebhooksClient().update_webhook(\n",
    "  id=http_webhook.id,\n",
    "  status=\"ACTIVE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc655474-c432-46df-8ad3-55e252f93687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the model to production stage in mlflow\n",
    "time.sleep(20)\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  stage='Production',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800b0a1-7d4a-4d82-8ef1-c544b92a248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_version_uri = \"models:/{model_name}/production\".format(model_name=model_name_mlflow)\n",
    "\n",
    "print(\"Loading PRODUCTION model stage with name: '{model_uri}'\".format(model_uri=model_version_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5850-b30b-4df0-8956-1dbef6e528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"machine_type\": \"n1-standard-2\",\n",
    "    \"description\": 'Serving a logistic regression model',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3be4b7-06a2-42e6-a224-bc4e1f9134e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add better configuration\n",
    "from mlflow import deployments\n",
    "\n",
    "client = deployments.get_deploy_client(\"google_cloud\")\n",
    "model_version_uri = \"models:/manually_registered_sklearn/production\"\n",
    "deployment = client.create_deployment(\n",
    "    name=model_name_mlflow,\n",
    "    model_uri=model_version_uri,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589d836-a794-4dc4-881e-dc31e899f2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
